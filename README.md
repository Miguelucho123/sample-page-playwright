# Sample Page Python + Playwright + Pytest Automation - Miguel Gutierrez

This project automates login test cases using Python + Playwright, leveraging the Page Object Model (POM), logging, and automatic screenshots in case of failure.

## Basic installation

Create a virtual environment and install the required packages:

```bash 
python -m venv venv
source venv/bin/activate  # In Mac/Linux
venv\Scripts\activate     # In Windows
```
Then install the required packages:

```bash
pip install -r requirements.txt
playwright install
```

## Run Tests

```bash
pytest -v
```

## Requirements

- Python 3.8+
- Playwright
- pytest

##REPORTS

![image](https://github.com/user-attachments/assets/fbca52b0-01f4-460f-8c39-179858824312)


## Questions and Answers
- **How would you integrate this test suite into a CI/CD pipeline (e.g., GitHub Actions,
Jenkins)?** _To integrate this suite into a pipeline, you must first create a configuration file that outlines the step-by-step process to install the project and its dependencies, followed by its execution and the generation of an artifact containing the results—in this case, Mochawesome reports. Once the configuration file is set up, if continuous testing is desired, the pipeline can be connected to the development pipelines to ensure that the automated test suite runs before each deployment._

- **What would be your approach to scaling this framework for a large application?:** _To scale this framework for a large application, I would follow a modular and sustainable approach based on automation best practices:_

    _**1. Modular Architecture:** I would implement the Page Object Model (POM) pattern to separate test logic from element interaction logic, which improves maintainability and code reuse._

    _**2. Data Separation:** I’d use external configuration and data files (like JSON or YAML) to decouple test logic from test data._

    _**3. Parallel and Distributed Execution:** I’d configure the framework to support parallel and cross-environment execution to reduce test run times, using tools like Cypress Dashboard, GitHub Actions Matrix, or Selenium Grid._

    _**4. Environment Management:** I would adapt the framework to dynamically handle multiple environments (QA, staging, production) through environment variables._

    _**5. Centralized Reporting and Logging:** I’d integrate advanced reporting tools like Mochawesome or Allure, along with robust logging mechanisms for better traceability and debugging._

   _**6. CI/CD Integration:** I’d ensure the framework is fully integrated into CI/CD pipelines so that every commit or pull request automatically triggers test executions._

   _**7. Layered Testing Strategy:** I would introduce different layers of testing (UI, API, integration, smoke) and use test tagging to allow selective execution based on priorities._

   _**8. Documentation and Standards:** I’d maintain clear documentation and enforce coding standards to make the framework collaborative and scalable across larger teams._

- **What quality metrics would you track and report on as a QA Leader?** _As a QA Leader, I would track and report on a combination of process, product, and testing quality metrics to provide a holistic view of the software’s health and the team's efficiency. These include:

	_**1. Test Coverage:** Percentage of code, features, or requirements covered by automated and manual tests._

	_**2. Defect Density:** Number of defects per module or lines of code, helping to identify risk areas._

	_**3. Test Pass/Fail Rate:** The ratio of passed vs. failed tests in each test cycle or CI pipeline._

	_**4. Defect Leakage:** Number of defects found in production vs. those found in testing, to measure test effectiveness._

	_**5. Time to Resolution:** Average time taken to fix and close bugs after detection._

	_**6. Automation Rate:** Percentage of test cases automated vs. total test cases._

	_**7. Regression Failure Rate:** Percentage of automated regression tests that fail after new code is introduced._

	_**8. Build Stability:** Number of successful vs. failed builds related to test failures._

	_**9. Test Execution Time:** Time taken to execute the full suite, especially relevant in CI/CD pipelines._

	_**10. Customer-reported Defects:** Number of issues reported by end-users, to evaluate real-world impact._

_These metrics help drive decisions on where to improve quality, optimize test strategies, and ensure continuous delivery with confidence._

